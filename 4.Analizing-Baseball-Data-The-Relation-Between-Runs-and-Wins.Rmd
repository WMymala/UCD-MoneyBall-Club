---
title: "4.-Analyzing-Baseball-Data-The-Relation-Between-Runs-and-Wins"
author: "Walter Mymala, Mihir Kulkarni"
output: html_document
---

Book: Analyzing Baseball Data with R (Second Edition) - Chapman and Hall

Download Files for code at: https://github.com/beanumber/baseball_R > Code (Green Button) > Download

Files used: 

* game_log_header.csv

* 

# Chapter 4

Stats really don't mean anything if you can't win. You cannot just compare player stats to determine who is better. If that was the case, Russell Westbrook would be the GOAT but in reality he is a rebound and assist merchant, aka a FRAUD.

# 4.2 The Teams Table in the Lahman Database

We want to relate the proportion of wins with the runs scored and runs allowed for all of the teams. The relevant fields of interest in this table are the number of games played G, the number of team wins W, the number of losses L, the total number of runs scored R, and the total number of runs allowed RA.
```{r}
library(tidyverse)
library(Lahman)
tail(Teams, 3) # Teams table shows seasonal stats for MLB teams going back to the first professional season in 1871
```

```{r}
# new data frame w/ 5 columns of interest. filter() allows us to focus on seasons since 2001
my_teams <- Teams %>%
  filter(yearID > 2000) %>%
  select(teamID, yearID, lgID, G, W, L, R, RA)
my_teams %>%
  tail()

# run differential - Difference between the runs scored and the runs allowed by a team.
my_teams <- my_teams %>%
  mutate(RD = R - RA, Wpct = W / (W + L)) # add 2 new variables to my_team: RD (run differential) and Wpct (winning percentage)

# scatterplot for RD and Wpct (we will add to this)
run_diff <- ggplot(my_teams, aes(x = RD, y = Wpct)) +
  geom_point() +
  scale_x_continuous("Run differential") +
  scale_y_continuous("Winning percentage")
run_diff
```

# 4.3 Linear Regression

One way to predict a team's winning prectanae using runs scored and allowed is linear regression $$Wpct = a + b × RD + ε,$$

* Wpct is the variable of interest (dependent/response variable)

* All the right-hand-side (RHS) variables (independent variables) influence Wpct in determining how much each variable correlates to Wpct. a and b are unknown constants and ε is the error term that captures all other factors influencing the response variable

```{r}
# lm() allows us to run linear models
linfit <- lm(Wpct ~ RD, data = my_teams)
linfit

# plot shows a strong, positive relationship—teams w/ large run differentials are more likely ro be winning
run_diff +
  geom_smooth(method = "lm", se = FALSE, color = "blue")
```

The linear regression equation is $Wpct = 0.499994 + 0.000626 × RD$. 

* A team with a run differential of zero (RD = 0) will win half of its games (estimated intercept ≈ .500). 

* a one-unit increase in run differential corresponds to an increase of 0.000626 in winning percentage

* a team scoring 750 runs and allowing 750 runs is predicted to win half of its games corresponding to 81 games in a typical MLB season of 162 games. In contrast, a team scoring 760 runs and allowing 750 has a run differential of +10 and is predicted to have a winning percentage of 0.500+10·0.000626 ≈ 0.506. A winning percentage of 0.506 in a 162-game schedule corresponds to 82 wins.

Once we have a fitted model, use augment() to calculate the predicted values from the model, as well as the residuals, which measure the difference between the response values and the fitted values. The residual plot is a representation of how close each data point is vertically from the graph of the prediction equation from the model. $Residual = Observed – Predicted$. A good residual plot consists of: 

* symmetrically distributed values, tending to cluster towards the middle of the plot.

* values clustered around the lower single digits of the y-axis (e.g., 0.5 or 1.5, not 30 or 150).

* in general, there aren’t any clear patterns.
```{r}
library(broom)
my_teams_aug <- augment(linfit, data = my_teams)

# set up residual plot for run differential
base_plot <- ggplot(my_teams_aug, aes(x = RD, y = .resid)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = 3) +
  xlab("Run differential") + ylab("Residual")

# gets 4 teams with the largest residuals(farthest values away from predictor line)
highlight_teams <- my_teams_aug %>%
  arrange(desc(abs(.resid))) %>%
  head(4)

# ggrepel adds labels gotten from highlight_teams
library(ggrepel)
base_plot +
  geom_point(data = highlight_teams, color = "blue") +
  geom_text_repel(data = highlight_teams, color = "blue",
                  aes(label = paste(teamID, yearID)))
```
Residuals here are interpreted as the error of the linear model in predicting the actual winning percentage. Points farthest from the zero line correspond to the teams where the linear model fared worst in predicting the winning percentage.

The 2006 Cleveland Indians, with a +88 run differential, were supposed to have a 0.555 team according to the linear equation, but they actually finished at 0.481, corresponding to the residual 0.481 − 0.555 = −0.073, or −11.8 games.

We want to see if errors are normally distributed. The average value of the residuals for any least squares linear model is equal to zero. This means that we want our model to be unbiased in that the model predictions are equally likely to overestimate and underestimate the winning percentage.
```{r}
# checking for bias using RMSE (root mean squared error)
resid_summary <- my_teams_aug %>%
  summarize(N = n(), avg = mean(.resid),
            RMSE = sqrt(mean(.resid^2)))
resid_summary
```

If the errors are normally distributed, approximately two thirds of the residuals fall between −RMSE and +RMSE, while 95% of the residuals are between −2·RMSE and 2·RMSE.
```{r}
# confirmation of statement above; n() and summarize() obtain the number of rows of a data frame. In the numerators of the expressions, we obtain the number of residuals (computed using the abs() function) that are smaller than one and two RMSE
rmse <- resid_summary %>%
  pull(RMSE)

# The computed fractions are close to the theoretical 68% and 95% values stated above
my_teams_aug %>%
  summarize(N = n(),
            within_one = sum(abs(.resid) < rmse),
            within_two = sum(abs(.resid) < 2 * rmse)) %>%
  mutate(within_one_pct = within_one / N,
         within_two_pct = within_two / N)
```

# 4.4 The Pythagorean Formula for Winning Percentage

Bill James, the godfather of sabermetrics, derived this non-linear formula to estimate winning percentage: $$Wpct = \frac{R2}{R^2 + {RA}^2}$$
```{r}
# adding Pythagorean expectation (name of the formula) to df 
my_teams <- my_teams %>%
  mutate(Wpct_pyt = R ^ 2 / (R ^ 2 + RA ^ 2))

# residuals_pyt is the difference between the actual and predicted winning percentages
my_teams <- my_teams %>%
  mutate(residuals_pyt = Wpct - Wpct_pyt)
# residuals need to be calculated explicitly; we compare RMSE (root-Mean squared error) for new predictions
my_teams %>%
  summarize(rmse = sqrt(mean(residuals_pyt^2)))
```

RMSE calculated on the Pythagorean predictions is similar in value to the one calculated with the linear predictions. They have similar rmses but the non-linear has better properties how predicting scenarios. For example, a team has pitchers who never allow runs, while the hitters always manage to score the only run they need. They score 162 runs in a season and win all of their games, but the linear equation would predict them to be merely a .601 team. The Pythagorean model instead correctly predicts that this team will win all of games.

Going back to the Pythagorean model, $$Wpct = \frac{R^2}{R%2 + {RA}^2}$$, it turns out 2 isn't the best exponent that would give the best fit for our dataframe and it's actually 1.85 instead. 

***** Optional things: (read this but don't have to): We rewrite formula as $$Wpct = {\frac{W}{W+L}}\frac{R^k}{R^k + {RA}^k$$ to get $$log(\frac{W}{L}) = k • log(\frac{R}{RA})$$ where $log(\frac{W}{L})$ is percentage of wins and losses. $k • log(\frac{R}{RA})$ is exponent times runs divided by percentage of runs over runs allowed.
```{r}
# add percent win rate and percent runs to runs allowed as variables
my_teams <- my_teams %>%
  mutate(logWratio = log(W / L),
         logRratio = log(R / RA))

# specify a model with a zero y-intercept by adding a zero term on the RHS
pytFit <- lm(logWratio ~ 0 + logRratio, data = my_teams)
pytFit # output says a best-fit Pythagorean exponent of 1.85
```
*****

Ex) The 2011 Boston Red Sox scored 875 runs, while allowing 737. According to the Pythagorean model with exponent 2, they were expected to win 95 games, but won 91. Pythagorean model isn't completly accurate in all cases.
```{r}
# gl2011.txt gives info on every game played in the 2011 season
glheaders <- read_csv("/Users/yeezywally_510/Desktop/MoneyBall/baseball_R-master/data/game_log_header.csv")
gl2011 <- read_csv("/Users/yeezywally_510/Desktop/MoneyBall/baseball_R-master/data/gl2011.txt",
                   col_names = names(glheaders),
                   na = character())
# commands select the lines pertaining to the Red Sox games and keep only the columns related to runs
BOS2011 <- gl2011 %>%
  filter(HomeTeam == "BOS" | VisitingTeam == "BOS") %>%
  select(VisitingTeam, HomeTeam,
         VisitorRunsScored, HomeRunsScore)
head(BOS2011)

# calculate run differentials (ScoreDiff) both for games won and lost and add a column W for if the Red Sox won the game
BOS2011 <- BOS2011 %>%
  mutate(ScoreDiff = ifelse(HomeTeam == "BOS", HomeRunsScore - VisitorRunsScored,
VisitorRunsScored - HomeRunsScore), W = ScoreDiff > 0)

# compute summary statistics on the run differentials for games won and for games lost. group_by() specifies grouping factor (i.e., whether the game resulted in a win for Boston). skim() takes a variable name and computes a host of relevant summary statistics
library(skimr)
BOS2011 %>%
  group_by(W) %>%
  skim(ScoreDiff)
```
The 2011 Red Sox had their victories decided by a larger margin than their losses (4.3 vs -3.5 runs on average), leading to underperformance of the Pythagorean prediction by five games. A team overperforming (or underperforming) its Pythagorean winning percentage is often seen as being lucky (or unlucky), and is expected to get closer to its expected line as the season progresses.

A team can overperform its Pythagorean winning percentage by winning a disproportionate number of close games
```{r}
# winner variable contains the abbreviation of the winning team and second variable diff contains the margin of victory
results <- gl2011 %>%
  select(VisitingTeam, HomeTeam,
         VisitorRunsScored, HomeRunsScore) %>%
  mutate(winner = ifelse(HomeRunsScore > VisitorRunsScored,
                         HomeTeam, VisitingTeam),
         diff = abs(VisitorRunsScored - HomeRunsScore))

# one_run_wins contains only the games decided by one run (we want games that are close). n() counts the number of wins in these games for each team
one_run_wins <- results %>%
  filter(diff == 1) %>%
  group_by(winner) %>%
  summarize(one_run_w = n())

# look at the relation between the Pythagorean residuals and the number of one-run victories. the team abbreviation for the Angels needs to be changed because it is coded as "LAA" in the Lahman database and as "ANA" in the Retrosheet game logs
teams2011 <- my_teams %>%
  filter(yearID == 2011) %>%
  mutate(teamID = ifelse(teamID == "LAA", "ANA",
                         as.character(teamID))) %>%
  inner_join(one_run_wins, by = c("teamID" = "winner"))

# plot
ggplot(data = teams2011, aes(x = one_run_w, y = residuals_pyt)) +
  geom_point() +
  geom_text_repel(aes(label = teamID)) +
  xlab("One run wins") + ylab("Pythagorean residuals")
```

* San Francisco had a large number of one-run victories and a large positive Pythogorean residual. In contrast, San Diego had few one-run victories and a negative residual (poverty xD)

* Winning a disproportionate number of close games is sometimes attributed to plain luck, but might be teams with superior closers are able to overperform their Pythagorean E(winning %)
```{r}
# filter() selects the pitcher-seasons where more than 50 games were finished by a pitcher with an ERA lower than 2.50. df top_closers has only the columns of the pitcher, the season, and the team
top_closers <- Pitching %>%
  filter(GF > 50 & ERA < 2.5) %>%
  select(playerID, yearID, teamID)

# merge the top_closers data frame with our my_teams dataset, cre- ating a data frame that contains the teams featuring a top closer. Then use summary statistics
my_teams %>%
  inner_join(top_closers) %>%
  pull(residuals_pyt) %>%
  summary()
```

# 4.5 How many runs for a Win?

Ten-runs-equal-one-win rule of thumb: ex) Suppose a team scores an average of five runs per game, while allowing the same number of runs. In a 162-game season, the team would score (and allow) 810 runs. Inserting 810 in the Pythagorean formula one gets (as expected) a perfect .500 expected winning percentage with 81 wins.

Similar formula to Pythagorean formula: $$W = G • \frac{R^2}{R^2 + {RA}^2}$$

The book does some boring proofs and code, but what's important is 

* the rule of ten is appropriate in typical run scoring environments (4 to 5 runs per game). 

* However, in very low scoring environments (> 4 runs per game), a lower number of runs is needed to gain an extra win; 

* in high scoring environments (< 5 runs per game), one needs a larger number of runs for an added win.

What's the big picture?: Linear regression lm() can be used to predict the value of interest using predictor variables but beware because it isn't always accurate! That's because there are residuals (errors like luck that affect the predicted value). The Pythagorean model $$Wpct = {\frac{W}{W+L}}\frac{R^k}{R^k + {RA}^k$$ is mostly accurate for predicting models except it isn't foolproof either because it doesn't take into account luck, close games, closer skill, etc.

No model for predicting is perfect basically




