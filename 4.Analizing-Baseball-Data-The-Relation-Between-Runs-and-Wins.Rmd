---
title: "4.-Analyzing-Baseball-Data-The-Relation-Between-Runs-and-Wins"
author: "Walter Mymala, Mihir Kulkarni"
output: html_document
---

Book: Analyzing Baseball Data with R (Second Edition) - Chapman and Hall

# Chapter 4

Stats really don't mean anything if you can't win. You cannot just compare player stats to determine who is better. If that was the case, Russell Westbrook would be the GOAT but in reality he is a rebound and assist merchant, aka a FRAUD.

# 4.2 The Teams Table in the Lahman Database

We want to relate the proportion of wins with the runs scored and runs allowed for all of the teams. The relevant fields of interest in this table are the number of games played G, the number of team wins W, the number of losses L, the total number of runs scored R, and the total number of runs allowed RA.
```{r}
library(tidyverse)
library(Lahman)
tail(Teams, 3) # Teams table shows seasonal stats for MLB teams going back to the first professional season in 1871
```

```{r}
# new data frame w/ 5 columns of interest. filter() allows us to focus on seasons since 2001
my_teams <- Teams %>%
  filter(yearID > 2000) %>%
  select(teamID, yearID, lgID, G, W, L, R, RA)
my_teams %>%
  tail()

# run differential - Difference between the runs scored and the runs allowed by a team.
my_teams <- my_teams %>%
  mutate(RD = R - RA, Wpct = W / (W + L)) # add 2 new variables to my_team: RD (run differential) and Wpct (winning percentage)

# scatterplot for RD and Wpct (we will add to this)
run_diff <- ggplot(my_teams, aes(x = RD, y = Wpct)) +
  geom_point() +
  scale_x_continuous("Run differential") +
  scale_y_continuous("Winning percentage")
run_diff
```

# 4.3 Linear Regression

One way to predict a team's winning prectanae using runs scored and allowed is linear regression $$Wpct = a + b × RD + ε,$$

* Wpct is the variable of interest (dependent/response variable)

* All the right-hand-side (RHS) variables (independent variables) influence Wpct in determining how much each variable correlates to Wpct. a and b are unknown constants and ε is the error term that captures all other factors influencing the response variable

```{r}
# lm() allows us to run linear models
linfit <- lm(Wpct ~ RD, data = my_teams)
linfit

# plot shows a strong, positive relationship—teams w/ large run differentials are more likely ro be winning
run_diff +
  geom_smooth(method = "lm", se = FALSE, color = "blue")
```

The linear regression equation is $Wpct = 0.499994 + 0.000626 × RD$. 

* A team with a run differential of zero (RD = 0) will win half of its games (estimated intercept ≈ .500). 

* a one-unit increase in run differential corresponds to an increase of 0.000626 in winning percentage

* a team scoring 750 runs and allowing 750 runs is predicted to win half of its games corresponding to 81 games in a typical MLB season of 162 games. In contrast, a team scoring 760 runs and allowing 750 has a run differential of +10 and is predicted to have a winning percentage of 0.500+10·0.000626 ≈ 0.506. A winning percentage of 0.506 in a 162-game schedule corresponds to 82 wins.

Once we have a fitted model, use augment() to calculate the predicted values from the model, as well as the residuals, which measure the difference between the response values and the fitted values. The residual plot is a representation of how close each data point is vertically from the graph of the prediction equation from the model. $Residual = Observed – Predicted$. A good residual plot consists of: 

* symmetrically distributed values, tending to cluster towards the middle of the plot.

* values clustered around the lower single digits of the y-axis (e.g., 0.5 or 1.5, not 30 or 150).

* in general, there aren’t any clear patterns.
```{r}
library(broom)
my_teams_aug <- augment(linfit, data = my_teams)

# set up residual plot for run differential
base_plot <- ggplot(my_teams_aug, aes(x = RD, y = .resid)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = 3) +
  xlab("Run differential") + ylab("Residual")

# gets 4 teams with the largest residuals(farthest values away from predictor line)
highlight_teams <- my_teams_aug %>%
  arrange(desc(abs(.resid))) %>%
  head(4)

# ggrepel adds labels gotten from highlight_teams
library(ggrepel)
base_plot +
  geom_point(data = highlight_teams, color = "blue") +
  geom_text_repel(data = highlight_teams, color = "blue",
                  aes(label = paste(teamID, yearID)))
```
Residuals here are interpreted as the error of the linear model in predicting the actual winning percentage. Points farthest from the zero line correspond to the teams where the linear model fared worst in predicting the winning percentage.

The 2006 Cleveland Indians, with a +88 run differential, were supposed to have a 0.555 team according to the linear equation, but they actually finished at 0.481, corresponding to the residual 0.481 − 0.555 = −0.073, or −11.8 games.

We want to see if errors are normally distributed. The average value of the residuals for any least squares linear model is equal to zero. This means that we want our model to be unbiased in that the model predictions are equally likely to overestimate and underestimate the winning percentage.
```{r}
# checking for bias using RMSE (root mean squared error)
resid_summary <- my_teams_aug %>%
  summarize(N = n(), avg = mean(.resid),
            RMSE = sqrt(mean(.resid^2)))
resid_summary
```

If the errors are normally distributed, approximately two thirds of the residuals fall between −RMSE and +RMSE, while 95% of the residuals are between −2·RMSE and 2·RMSE.
```{r}
# confirmation of statement above; n() and summarize() obtain the number of rows of a data frame. In the numerators of the expressions, we obtain the number of residuals (computed using the abs() function) that are smaller than one and two RMSE
rmse <- resid_summary %>%
  pull(RMSE)

# The computed fractions are close to the theoretical 68% and 95% values stated above
my_teams_aug %>%
  summarize(N = n(),
            within_one = sum(abs(.resid) < rmse),
            within_two = sum(abs(.resid) < 2 * rmse)) %>%
  mutate(within_one_pct = within_one / N,
         within_two_pct = within_two / N)
```

# 4.4 The Pythagorean Formula for Winning Percentage

Bill James, the godfather of sabermetrics, derived this non-linear formula to estimate winning percentage: $$Wpct = \frac{R2}{R^2 + {RA}^2}$$
```{r}
# adding Pythagorean expectation (name of the formula) to df 
my_teams <- my_teams %>%
  mutate(Wpct_pyt = R ^ 2 / (R ^ 2 + RA ^ 2))

# residuals_pyt is the difference between the actual and predicted winning percentages
my_teams <- my_teams %>%
  mutate(residuals_pyt = Wpct - Wpct_pyt)
# residuals need to be calculated explicitly; we compare RMSE (root-Mean squared error) for new predictions
my_teams %>%
  summarize(rmse = sqrt(mean(residuals_pyt^2)))
```

RMSE calculated on the Pythagorean predictions is similar in value to the one calculated with the linear predictions. They have similar rmses but the non-linear has better properties how predicting scenarios. For example, a team has pitchers who never allow runs, while the hitters always manage to score the only run they need. They score 162 runs in a season and win all of their games, but the linear equation would predict them to be merely a .601 team. The Pythagorean model instead correctly predicts that this team will win all of games.

Going back to the Pythagorean model, $$Wpct = \frac{R^2}{R%2 + {RA}^2}$$, it turns out 2 isn't the best exponent that would give the best fit for our dataframe and it's actually 1.85 instead. 

Optional (read this but don't have to): We rewrite formula as $$Wpct = {\frac{W}{W+L}}\frac{R^k}{R^k + {RA}^k$$
```{r}
# R output suggests a best-fit Pythagorean exponent of 1.85
my_teams <- my_teams %>%
  mutate(logWratio = log(W / L),
         logRratio = log(R / RA))
pytFit <- lm(logWratio ~ 0 + logRratio, data = my_teams)
pytFit
```


